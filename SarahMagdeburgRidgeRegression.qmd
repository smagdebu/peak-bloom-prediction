---
title: "Cherry Blossom 2026 Ridge Regression Predictions"
author: "Sarah Magdeburg"
date: "02/28/2026"
lang: en-US
format:
  html:
    embed-resources: true
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, 
                      message = FALSE,
                      fig.align = 'center',
                      out.width = '80%')
```


## Introduction

I will predicting the peak bloom date for all the competition's five locations using a ridge regression. This ridge regression will be fit based on the temperatures of the 25 days after the last frost of the winter/spring. Historic temperature data is gotten via the [NOAA web API](https://www.ncdc.noaa.gov/cdo-web/) and forecasted 2026 temperature data is gotten from Accuweather.

I will be using methods from the _tidyverse_ of R packages.

```{r}
library(tidyverse)
```

## Loading the data

The data for the five sites is provided by the competition as simple text files in CSV format. I read the data into R in the same fashion as the demo code.

```{r}
cherry <- read.csv("data/washingtondc.csv") |> 
  bind_rows(read.csv("data/liestal.csv")) |> 
  bind_rows(read.csv("data/kyoto.csv")) |> 
  bind_rows(read.csv("data/vancouver.csv")) |> 
  bind_rows(read.csv("data/nyc.csv"))
```


## Predicting the peak bloom

This helper function is provided by the competition's demo analysis.

```{r}
#' Small helper function to convert the day of year to
#' the actual date.
#' 
#' @param year year as an integer
#' @param doy day of the year as integer (1 means January 1st)
#' @return date string
doy_to_date <- function (year, doy) {
  strptime(paste(year, doy, sep = '-'), '%Y-%j') |> # create date object
    strftime('%Y-%m-%d') # translate back to date string in ISO 8601 format
}
```


## Collecting temperature data

My predictions are based upon global meteorological data from the Global Historical Climatology Network (GHCN), available through the NOAA web API.

To use the web API, you first need a web service token.
You can request this token (free of charge) via <https://www.ncdc.noaa.gov/cdo-web/token>.
Once you have been issued the token, note it somewhere in your code (or make it available through an environment variable):

```{r}
NOAA_WEB_API_TOKEN <- 'VGerAgVfgPaylgClNTVibLsaiLYYPZGl'
```

To connect to and use the web API I uses the following R packages:

```{r}
library(httr2)
library(jsonlite)
```

The stations closest to the sites for the competition with continuously collected maximum temperatures are USC00186350 (Washington D.C.), GME00127786 (Liestal), JA000047759 (Kyoto), CA001108395 (Vancouver) and USW00014732 (New York City).

```{r}
NOAA_API_BASE_URL <- "https://www.ncei.noaa.gov/cdo-web/api/v2/data"

# Station IDs for the specified locations
stations <- c(
  "washingtondc" = "GHCND:USC00186350",
  "vancouver"    = "GHCND:CA001108395",
  "newyorkcity"  = "GHCND:USW00014732",
  "liestal"      = "GHCND:GME00127786",
  "kyoto"        = "GHCND:JA000047759")
```

I will be retrieving the maximum and minimum daily temperatures (in °C) from these stations using the `get_temperature()` function defined in the demo code for this competition, which wraps the `ghcnd_search()` function in the `rnoaa` package. (N.b. `ghcnd_search()` returns a list. Each element of the list corresponds to an element of the `var` argument.)

```{r}
nested_to_tibble <- function (x) {
  # Determine the variable names in the response
  variable_names <- map(x, names) |> 
    unlist(use.names = FALSE) |> 
    unique()
  
  names(variable_names) <- variable_names

  # Reshape the response from a nested list into a table
  map(variable_names, \(i) {
    map(x, \(y) {
      if (is.null(y[[i]])) {
        NA_character_
      } else {
        y[[i]]
      }
    }) |> 
      unlist(use.names = FALSE)
  }) |> 
    as_tibble()
}

get_daily_avg_temp <- function(station_id, start_date, end_date,
                               api_key, base_url, window_size = 300) {
  windows <- seq(as_date(start_date),
                 as_date(end_date) + days(window_size + 1),
                 by = sprintf("%d days", window_size))
  
  batches <- map2(windows[-length(windows)], windows[-1] - days(1), \(from, to) {
    if (from > Sys.Date()) {
      return(NULL)
    }
    response <- tryCatch(
      request(base_url) |> 
        req_headers(token = api_key) |> 
        req_url_query(
          datasetid = "GHCND",
          stationid = station_id,
          datatypeid = "TMIN,TMAX",
          startdate = from,
          enddate = min(as_date(to), Sys.Date()),
          units = "metric",
          limit = 1000
        ) |> 
        req_retry(max_tries = 10) |> 
        req_perform() |> 
        resp_body_json(),
      
      httr2_http = \(cnd) {
        rlang::warn(sprintf("Failed to retrieve data for station %s in time window %s--%s",
                            station_id, from, to),
                    parent = cnd)
        NULL
      })
  })
  
  map(batches, \(x) nested_to_tibble(x$results)) |> 
    list_rbind() |> 
    mutate(date = as_date(date))
}
```

Collect minimum and maximum temperatures from 1988 to the present. This code, like the code for interacting with the API, is borrowed from the competition's demo code.

```{r}
#| cache: true
historic_temperatures <- cherry |> 
  group_by(location) |> 
  summarize(start_date = sprintf('%d-01-01', pmax(1989, min(year)) - 1)) |> 
  left_join(tibble(location = names(stations),
                   station_id = stations),
            by = 'location') |> 
  group_by(location) |> 
  group_modify(\(x, gr) {
    get_daily_avg_temp(station_id = x$station_id,
                       start_date = x$start_date,
                       end_date = Sys.Date(),
                       api_key = NOAA_WEB_API_TOKEN,
                       base_url = NOAA_API_BASE_URL)
  })
```

Separate the collected data into minimum and maximum temperatures

```{r}
max_temps <- historic_temperatures |>
  filter(datatype == "TMAX") |>
  select(location, date, value) |>
  rename(tmax = value)

min_temps <- historic_temperatures |>
  filter(datatype == "TMIN") |>
  select(location, date, value) |>
  rename(tmin = value)
```

As observed in the plots of the minimum and maximum temperatures, there are some gaps in the NOAA data for both maximum and minimum temperatures. This can affect the selection of observations for training. These plots are created using code from the competition's demo code.

```{r}
historic_temperatures |>
  filter(datatype == 'TMIN') |> 
  ggplot(aes(x = date, y = value)) + 
  geom_line() +
  labs(x = "Year", y = "Average minimum temperature (°C)") +
  facet_grid(rows = vars(location))
```


```{r}
historic_temperatures |>
  filter(datatype == 'TMAX') |> 
  ggplot(aes(x = date, y = value)) + 
  geom_line() +
  labs(x = "Year", y = "Average maximum temperature (°C)") +
  facet_grid(rows = vars(location))
```

I joined the maximum and minimum temperatures so that each single row in the dataset corresponds to a day. I used the date to calculate the day of the year. Then, I filtered the dataset to only include the days from January to May (May 31 is typically day 151 but day 152 on a leap year), as the 2026 weather data will only go up to May 31.

```{r}
combine_max_min <- inner_join(max_temps, min_temps,
                              by = c("location", "date")) |>
  mutate(doy = as.integer(strftime(date, "%j")), year = strftime(date, "%Y")) |>
  filter(doy <= 152)
```  

Then I identified the last frost for each year. I define the last frost as the last day from January 1 to May 31 that has a minimum temperature at or below freezing. I select observations between January and May that are the first 25 days after that year's last frost. I calculated an average temperature for each of the retained days using the daily minimum and maximum temperatures.

```{r}
last_frosts <- combine_max_min |>
  filter(tmin <= 0) |>
  group_by(location, year) |>
  summarise(lastfrost = max(doy))

temps_with_last_frost <- inner_join(combine_max_min, last_frosts,
                                    by = c("location", "year")) |>
  filter(doy > lastfrost) |>
  mutate(dslf = doy - lastfrost) |>
  filter(dslf <= 25) |>
  mutate(tavg = (tmax + tmin) / 2) |>
  select(location, year, dslf, tavg)
```

I pivot the days since last frost into columns so that these days can be used as predictors.

```{r}
transposed_temps <- temps_with_last_frost |>
  pivot_wider(id_cols = c(location, year), names_from = dslf, 
              names_glue = "{dslf}_temp", values_from = tavg) |>
  drop_na()

transposed_temps$year <- as.integer(transposed_temps$year)
```

By this point, some observations have been lost. This is due to stations not having minimum and maximum temperatures consistently available during the 25-day frame after the last frost. I join these days and their temperatures to the peak bloom dates data to form my training data.

```{r}  
temps_with_bloom <- inner_join(cherry, transposed_temps,
                               by = c("location", "year"))
```

## Building predictive model

I am using ridge regression to predict the peak bloom dates. Ridge regression is typically performed using the __glmnet__ package. The glmnet package uses a vector response y and matrix x to fit the model. I split the peak bloom date and the days predictors accordingly into the response vector and predictor matrix.

```{r}
library(glmnet)

bloom <- temps_with_bloom$bloom_doy
temps_only <- temps_with_bloom |>
  select(!c("lat", "long", "alt", "bloom_date", "bloom_doy", "location", "year"))
temps_matrix <- as.matrix(temps_only)
```

Setting a seed for reproducibility of results, cross-validation is used to find the optimal lambda for the ridge regression. I use 5-fold cross-validation instead of the default 10-fold cross-validation due to the small number of observations.

```{r}
set.seed(490)
ridge.mod <- cv.glmnet(temps_matrix, bloom, alpha = 0, nfolds = 5)
bestlam.ridge <- ridge.mod$lambda.min
best.ridge.mod <- glmnet(temps_matrix, bloom, alpha = 0, lambda = bestlam.ridge)

prediction_train <- predict(best.ridge.mod, s = bestlam.ridge, newx = temps_matrix)
mse_ridge <- mean((bloom - prediction_train) ** 2)
mse_ridge

head(bloom)
head(prediction_train)
```

Observing the results of applying the training data to the fitted model and comparing it to the actual peak bloom dates provides a sense of the model's accuracy. I also calculated the MSE of reapplying the training data to the ridge regression model. 

## Making 2026 predictions

First, the temperature data for each location must be prepared and arranged in a similar manner to the training data. While the training data was obtained from the NOAA API, the 2026 weather data is instead read from a CSV that has temperature data from Accuweather (as provided by the competition). Last frost and the subsequent steps applied to the training data are also done here.

```{r}
data_2026 <- read.csv("data/accuweather_forecast_2026.csv")

prediction_data <- data_2026 |>
  mutate(doy = as.integer(strftime(date, "%j"))) |>
  filter(doy <= 152) 

last_frosts_prediction <- prediction_data |>
  filter(tmin <= 0) |>
  group_by(location, year) |>
  summarise(lastfrost = max(doy))

predictions_with_last_frost <- inner_join(prediction_data, last_frosts_prediction,
                                    by = c("location", "year")) |>
  filter(doy > lastfrost) |>
  mutate(dslf = doy - lastfrost) |>
  filter(dslf <= 25)

transposed_prediction_temps <- predictions_with_last_frost |>
  pivot_wider(id_cols = c(location, year), names_from = dslf,
              values_from = temp)
```

The 2026 data is now provided to the fitted ridge regression model to predict.

```{r}
locations <- c("kyoto", "liestal", "newyork", "vancouver", "washingtondc")
year <- rep("2026", 5)
dates <- rep(0, 5)

kyoto_2026 <- as.matrix(transposed_prediction_temps[1, 3:27])
kyoto_prediction <- predict(best.ridge.mod, s = bestlam.ridge, newx = kyoto_2026)
dates[1] <- doy_to_date(2026, kyoto_prediction)

liestal_2026 <- as.matrix(transposed_prediction_temps[2, 3:27])
liestal_prediction <- predict(best.ridge.mod, s = bestlam.ridge, newx = liestal_2026)
dates[2] <- doy_to_date(2026, liestal_prediction)

newyork_2026 <- as.matrix(transposed_prediction_temps[3, 3:27])
newyork_prediction <- predict(best.ridge.mod, s = bestlam.ridge, newx = newyork_2026)
dates[3] <- doy_to_date(2026, newyork_prediction)

vancouver_2026 <- as.matrix(transposed_prediction_temps[4, 3:27])
vancouver_prediction <- predict(best.ridge.mod, s = bestlam.ridge, newx = vancouver_2026)
dates[4] <- doy_to_date(2026, vancouver_prediction)

washington_2026 <- as.matrix(transposed_prediction_temps[5, 3:27])
washington_prediction <- predict(best.ridge.mod, s = bestlam.ridge, newx = washington_2026)
dates[5] <- doy_to_date(2026, washington_prediction)
```

## Submission

My final predictions for the peak bloom date at each location is summarized below.

```{r}
predictions <- data.frame(locations, year, dates)
predictions
```
